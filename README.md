# Autograd
Autograd implementation by following Andrej Karpathy's tutorial see https://www.youtube.com/watch?v=VMj-3S1tku0&t=6939s
added a few extra things like zero_grad() and apply_gradients()


I wanted a deeper understanding of how ML frameworks are implemented and this was the perfect introduction. I also learned a lot about the backpropagation algorithm, chain rule, and simple derivatives in a practical way. This gave me some much-needed intuition about learning rates, vanishing/exploding gradients, and how added complexity of simple MLPs can converge to universal function approximators.
